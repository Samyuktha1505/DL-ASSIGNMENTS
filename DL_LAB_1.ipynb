{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwtA3SViARB7",
        "outputId": "a258f5a3-27ae-49ac-d123-ab1d6d859556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A:\n",
            " [[1. 2.]\n",
            " [3. 4.]]\n",
            "Matrix B:\n",
            " [[5. 6.]\n",
            " [7. 8.]]\n",
            "\n",
            "A + B:\n",
            " [[ 6.  8.]\n",
            " [10. 12.]]\n",
            "\n",
            "A - B:\n",
            " [[-4. -4.]\n",
            " [-4. -4.]]\n",
            "\n",
            "A * B:\n",
            " [[19. 22.]\n",
            " [43. 50.]]\n",
            "\n",
            "Transpose of A:\n",
            " [[1. 3.]\n",
            " [2. 4.]]\n",
            "\n",
            "Determinant of A: -2.0\n",
            "\n",
            "Inverse of A:\n",
            " [[-2.0000002   1.0000001 ]\n",
            " [ 1.5000001  -0.50000006]]\n",
            "\n",
            "Eigenvalues of A: [-0.37228122+0.j  5.372281  +0.j]\n"
          ]
        }
      ],
      "source": [
        "# 2nd\n",
        "import tensorflow as tf\n",
        "\n",
        "A = tf.constant([[1., 2.],\n",
        "                 [3., 4.]])\n",
        "B = tf.constant([[5., 6.],\n",
        "                 [7., 8.]])\n",
        "\n",
        "print(\"Matrix A:\\n\", A.numpy())\n",
        "print(\"Matrix B:\\n\", B.numpy())\n",
        "\n",
        "print(\"\\nA + B:\\n\", tf.add(A, B).numpy())\n",
        "\n",
        "print(\"\\nA - B:\\n\", tf.subtract(A, B).numpy())\n",
        "\n",
        "print(\"\\nA * B:\\n\", tf.matmul(A, B).numpy())\n",
        "\n",
        "print(\"\\nTranspose of A:\\n\", tf.transpose(A).numpy())\n",
        "\n",
        "print(\"\\nDeterminant of A:\", tf.linalg.det(A).numpy())\n",
        "\n",
        "print(\"\\nInverse of A:\\n\", tf.linalg.inv(A).numpy())\n",
        "\n",
        "e_vals, e_vecs = tf.linalg.eig(A)\n",
        "print(\"\\nEigenvalues of A:\", e_vals.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7RjL-c_hE6Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd\n",
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, lr=0.1, epochs=20):\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def train(self, X, y):\n",
        "        self.w = np.zeros(X.shape[1] + 1)\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            for xi, target in zip(X, y):\n",
        "                z = np.dot(xi, self.w[1:]) + self.w[0]\n",
        "                pred = 1 if z >= 0 else 0\n",
        "                update = self.lr * (target - pred)\n",
        "                self.w[1:] += update * xi\n",
        "                self.w[0] += update\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [1 if (np.dot(x, self.w[1:]) + self.w[0]) >= 0 else 0 for x in X]\n",
        "\n",
        "\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "\n",
        "y_and = np.array([0,0,0,1])\n",
        "p_and = Perceptron()\n",
        "p_and.train(X, y_and)\n",
        "print(\"AND Gate:\", p_and.predict(X))\n",
        "\n",
        "y_or = np.array([0,1,1,1])\n",
        "p_or = Perceptron()\n",
        "p_or.train(X, y_or)\n",
        "print(\"OR Gate:\", p_or.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmK8IT2jAX9z",
        "outputId": "f424b43c-1abd-4205-a95b-2daba5caf3ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AND Gate: [0, 0, 0, 1]\n",
            "OR Gate: [0, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4th\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "X = torch.tensor([[0.,0.],\n",
        "                  [0.,1.],\n",
        "                  [1.,0.],\n",
        "                  [1.,1.]])\n",
        "y = torch.tensor([[0.],[1.],[1.],[0.]])\n",
        "\n",
        "class XORNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(2, 2)\n",
        "        self.fc2 = nn.Linear(2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "model = XORNet()\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
        "\n",
        "for epoch in range(2000):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(X)\n",
        "    loss = criterion(out, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "    preds = model(X)\n",
        "    print(\"XOR Output:\")\n",
        "    print(torch.round(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHtr53cHAc5s",
        "outputId": "748c0328-9e2d-493a-d89d-16f8d96b4328"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XOR Output:\n",
            "tensor([[0.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5th\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"house_price_full+(2) - house_price_full+(2).csv\")\n",
        "\n",
        "X = df[['bedrooms', 'sqft_living']].values\n",
        "y = df['price'].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(2, activation='relu', input_shape=(2,)),\n",
        "    Dense(1, activation='linear')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(\"Test MAE:\", mae)\n",
        "\n",
        "sample = np.array([[3, 1500]])\n",
        "sample_scaled = scaler.transform(sample)\n",
        "prediction = model.predict(sample_scaled)\n",
        "\n",
        "print(\"Predicted House Price:\", prediction[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7wZZBMeC_3J",
        "outputId": "a6b94ffb-788c-4174-930f-729b276b2a6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 416304562176.0000 - mae: 551675.8750 - val_loss: 552508522496.0000 - val_mae: 594698.9375\n",
            "Epoch 2/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 429996408832.0000 - mae: 565003.1875 - val_loss: 552508522496.0000 - val_mae: 594698.9375\n",
            "Epoch 3/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 443817492480.0000 - mae: 558466.0625 - val_loss: 552508456960.0000 - val_mae: 594698.8750\n",
            "Epoch 4/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 443941552128.0000 - mae: 573042.0000 - val_loss: 552508456960.0000 - val_mae: 594698.8750\n",
            "Epoch 5/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 419767779328.0000 - mae: 553483.2500 - val_loss: 552508456960.0000 - val_mae: 594698.7500\n",
            "Epoch 6/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 395313774592.0000 - mae: 536702.8750 - val_loss: 552508391424.0000 - val_mae: 594698.7500\n",
            "Epoch 7/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 463691874304.0000 - mae: 584590.5625 - val_loss: 552508325888.0000 - val_mae: 594698.6875\n",
            "Epoch 8/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 459169071104.0000 - mae: 571457.3750 - val_loss: 552508325888.0000 - val_mae: 594698.6250\n",
            "Epoch 9/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 394286399488.0000 - mae: 531422.5000 - val_loss: 552508194816.0000 - val_mae: 594698.6250\n",
            "Epoch 10/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 431720529920.0000 - mae: 565554.0625 - val_loss: 552508194816.0000 - val_mae: 594698.6250\n",
            "Epoch 11/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 467824410624.0000 - mae: 582714.7500 - val_loss: 552508194816.0000 - val_mae: 594698.5625\n",
            "Epoch 12/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 446344331264.0000 - mae: 564126.0000 - val_loss: 552508129280.0000 - val_mae: 594698.5000\n",
            "Epoch 13/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 433088397312.0000 - mae: 559386.5000 - val_loss: 552508063744.0000 - val_mae: 594698.5000\n",
            "Epoch 14/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 400999940096.0000 - mae: 553577.4375 - val_loss: 552507998208.0000 - val_mae: 594698.4375\n",
            "Epoch 15/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 360006320128.0000 - mae: 525853.8750 - val_loss: 552507998208.0000 - val_mae: 594698.3750\n",
            "Epoch 16/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 428882198528.0000 - mae: 562468.0000 - val_loss: 552507932672.0000 - val_mae: 594698.3750\n",
            "Epoch 17/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 451901587456.0000 - mae: 572259.8125 - val_loss: 552507932672.0000 - val_mae: 594698.3125\n",
            "Epoch 18/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 410578812928.0000 - mae: 549353.6250 - val_loss: 552507867136.0000 - val_mae: 594698.3125\n",
            "Epoch 19/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 398458060800.0000 - mae: 543742.1250 - val_loss: 552507801600.0000 - val_mae: 594698.2500\n",
            "Epoch 20/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 394017505280.0000 - mae: 537080.4375 - val_loss: 552507801600.0000 - val_mae: 594698.1250\n",
            "Epoch 21/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 380435103744.0000 - mae: 537710.0000 - val_loss: 552507736064.0000 - val_mae: 594698.1250\n",
            "Epoch 22/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 397290176512.0000 - mae: 540361.1250 - val_loss: 552507670528.0000 - val_mae: 594698.1250\n",
            "Epoch 23/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 376860770304.0000 - mae: 533897.8750 - val_loss: 552507670528.0000 - val_mae: 594698.0000\n",
            "Epoch 24/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 401737252864.0000 - mae: 550029.2500 - val_loss: 552507670528.0000 - val_mae: 594698.0000\n",
            "Epoch 25/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 379896102912.0000 - mae: 530533.3750 - val_loss: 552507604992.0000 - val_mae: 594698.0000\n",
            "Epoch 26/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 433843175424.0000 - mae: 558765.6250 - val_loss: 552507473920.0000 - val_mae: 594698.0000\n",
            "Epoch 27/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 457639526400.0000 - mae: 564830.6250 - val_loss: 552507473920.0000 - val_mae: 594697.8750\n",
            "Epoch 28/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 516634476544.0000 - mae: 597156.3750 - val_loss: 552507408384.0000 - val_mae: 594697.8750\n",
            "Epoch 29/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 473221758976.0000 - mae: 578460.9375 - val_loss: 552507408384.0000 - val_mae: 594697.8125\n",
            "Epoch 30/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 476767715328.0000 - mae: 579630.0000 - val_loss: 552507408384.0000 - val_mae: 594697.8125\n",
            "Epoch 31/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 375210868736.0000 - mae: 529138.2500 - val_loss: 552507342848.0000 - val_mae: 594697.8125\n",
            "Epoch 32/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 420102471680.0000 - mae: 554561.5625 - val_loss: 552507277312.0000 - val_mae: 594697.6250\n",
            "Epoch 33/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 458313793536.0000 - mae: 560151.5000 - val_loss: 552507211776.0000 - val_mae: 594697.6250\n",
            "Epoch 34/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 431128707072.0000 - mae: 562607.6875 - val_loss: 552507211776.0000 - val_mae: 594697.6250\n",
            "Epoch 35/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 484788043776.0000 - mae: 587085.3125 - val_loss: 552507211776.0000 - val_mae: 594697.5000\n",
            "Epoch 36/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 425887924224.0000 - mae: 556853.8750 - val_loss: 552507146240.0000 - val_mae: 594697.5000\n",
            "Epoch 37/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 491379589120.0000 - mae: 590727.6875 - val_loss: 552507080704.0000 - val_mae: 594697.4375\n",
            "Epoch 38/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 418315337728.0000 - mae: 557520.4375 - val_loss: 552507015168.0000 - val_mae: 594697.3750\n",
            "Epoch 39/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 478168154112.0000 - mae: 575959.4375 - val_loss: 552506949632.0000 - val_mae: 594697.3750\n",
            "Epoch 40/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 400793272320.0000 - mae: 547601.9375 - val_loss: 552506884096.0000 - val_mae: 594697.3750\n",
            "Epoch 41/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 451707076608.0000 - mae: 566852.7500 - val_loss: 552506884096.0000 - val_mae: 594697.3125\n",
            "Epoch 42/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 414315053056.0000 - mae: 546199.6250 - val_loss: 552506884096.0000 - val_mae: 594697.3125\n",
            "Epoch 43/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 365326237696.0000 - mae: 524283.9688 - val_loss: 552506884096.0000 - val_mae: 594697.1250\n",
            "Epoch 44/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 466889605120.0000 - mae: 586593.2500 - val_loss: 552506753024.0000 - val_mae: 594697.1250\n",
            "Epoch 45/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 442099892224.0000 - mae: 570971.2500 - val_loss: 552506687488.0000 - val_mae: 594697.1250\n",
            "Epoch 46/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 411104772096.0000 - mae: 555975.1250 - val_loss: 552506687488.0000 - val_mae: 594697.0625\n",
            "Epoch 47/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 419486793728.0000 - mae: 556688.1250 - val_loss: 552506621952.0000 - val_mae: 594697.0000\n",
            "Epoch 48/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 413881303040.0000 - mae: 557517.7500 - val_loss: 552506621952.0000 - val_mae: 594696.8750\n",
            "Epoch 49/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 463309275136.0000 - mae: 580975.0000 - val_loss: 552506490880.0000 - val_mae: 594696.8750\n",
            "Epoch 50/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 475157921792.0000 - mae: 580075.1250 - val_loss: 552506490880.0000 - val_mae: 594696.8750\n",
            "Epoch 51/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 412863725568.0000 - mae: 546042.3750 - val_loss: 552506425344.0000 - val_mae: 594696.8750\n",
            "Epoch 52/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 412884074496.0000 - mae: 555525.1250 - val_loss: 552506359808.0000 - val_mae: 594696.8125\n",
            "Epoch 53/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 457969696768.0000 - mae: 577949.4375 - val_loss: 552506359808.0000 - val_mae: 594696.6875\n",
            "Epoch 54/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 426582179840.0000 - mae: 562343.4375 - val_loss: 552506294272.0000 - val_mae: 594696.6250\n",
            "Epoch 55/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 414539546624.0000 - mae: 551794.2500 - val_loss: 552506228736.0000 - val_mae: 594696.6250\n",
            "Epoch 56/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 455357104128.0000 - mae: 570738.1875 - val_loss: 552506228736.0000 - val_mae: 594696.6250\n",
            "Epoch 57/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 383441240064.0000 - mae: 541757.6875 - val_loss: 552506163200.0000 - val_mae: 594696.5000\n",
            "Epoch 58/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 443835973632.0000 - mae: 566653.3125 - val_loss: 552506097664.0000 - val_mae: 594696.5000\n",
            "Epoch 59/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 403102597120.0000 - mae: 548284.6250 - val_loss: 552506032128.0000 - val_mae: 594696.3750\n",
            "Epoch 60/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 392028585984.0000 - mae: 536648.6875 - val_loss: 552506032128.0000 - val_mae: 594696.3125\n",
            "Epoch 61/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 418165751808.0000 - mae: 546873.8125 - val_loss: 552505966592.0000 - val_mae: 594696.3125\n",
            "Epoch 62/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 466097897472.0000 - mae: 566742.1250 - val_loss: 552505901056.0000 - val_mae: 594696.3125\n",
            "Epoch 63/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 394845716480.0000 - mae: 540478.2500 - val_loss: 552505835520.0000 - val_mae: 594696.1875\n",
            "Epoch 64/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 404969029632.0000 - mae: 543799.7500 - val_loss: 552505769984.0000 - val_mae: 594696.1250\n",
            "Epoch 65/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 449088978944.0000 - mae: 561266.2500 - val_loss: 552505704448.0000 - val_mae: 594696.0625\n",
            "Epoch 66/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 424337047552.0000 - mae: 560181.6250 - val_loss: 552505638912.0000 - val_mae: 594696.0625\n",
            "Epoch 67/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 409203900416.0000 - mae: 553238.1250 - val_loss: 552505573376.0000 - val_mae: 594695.8750\n",
            "Epoch 68/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 497748836352.0000 - mae: 589244.3750 - val_loss: 552505573376.0000 - val_mae: 594695.8750\n",
            "Epoch 69/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 406161063936.0000 - mae: 555101.6875 - val_loss: 552505507840.0000 - val_mae: 594695.8125\n",
            "Epoch 70/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 454110150656.0000 - mae: 566516.1250 - val_loss: 552505442304.0000 - val_mae: 594695.8125\n",
            "Epoch 71/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 454416007168.0000 - mae: 574047.1250 - val_loss: 552505376768.0000 - val_mae: 594695.6875\n",
            "Epoch 72/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 430599569408.0000 - mae: 558587.1250 - val_loss: 552505376768.0000 - val_mae: 594695.6875\n",
            "Epoch 73/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 402028462080.0000 - mae: 535306.2500 - val_loss: 552505311232.0000 - val_mae: 594695.5625\n",
            "Epoch 74/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 419219406848.0000 - mae: 548355.8750 - val_loss: 552505245696.0000 - val_mae: 594695.5000\n",
            "Epoch 75/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 468355612672.0000 - mae: 576200.9375 - val_loss: 552505245696.0000 - val_mae: 594695.5000\n",
            "Epoch 76/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 414651908096.0000 - mae: 548867.6250 - val_loss: 552505180160.0000 - val_mae: 594695.3750\n",
            "Epoch 77/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 473408929792.0000 - mae: 577468.4375 - val_loss: 552505049088.0000 - val_mae: 594695.3750\n",
            "Epoch 78/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 424763490304.0000 - mae: 560939.2500 - val_loss: 552505049088.0000 - val_mae: 594695.3125\n",
            "Epoch 79/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 432020553728.0000 - mae: 564576.0000 - val_loss: 552504983552.0000 - val_mae: 594695.1875\n",
            "Epoch 80/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 378120404992.0000 - mae: 519416.6875 - val_loss: 552504983552.0000 - val_mae: 594695.1250\n",
            "Epoch 81/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 498663686144.0000 - mae: 588129.9375 - val_loss: 552504786944.0000 - val_mae: 594695.1250\n",
            "Epoch 82/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 422321389568.0000 - mae: 555593.3125 - val_loss: 552504786944.0000 - val_mae: 594695.0000\n",
            "Epoch 83/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 388765908992.0000 - mae: 540057.3750 - val_loss: 552504786944.0000 - val_mae: 594694.9375\n",
            "Epoch 84/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 419590373376.0000 - mae: 554314.1250 - val_loss: 552504655872.0000 - val_mae: 594694.8750\n",
            "Epoch 85/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 389558730752.0000 - mae: 542687.8750 - val_loss: 552504524800.0000 - val_mae: 594694.8125\n",
            "Epoch 86/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 466114019328.0000 - mae: 572768.6875 - val_loss: 552504524800.0000 - val_mae: 594694.7500\n",
            "Epoch 87/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 456394833920.0000 - mae: 569403.8125 - val_loss: 552504524800.0000 - val_mae: 594694.6875\n",
            "Epoch 88/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 445929848832.0000 - mae: 569850.2500 - val_loss: 552504393728.0000 - val_mae: 594694.6250\n",
            "Epoch 89/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 399255961600.0000 - mae: 544005.2500 - val_loss: 552504328192.0000 - val_mae: 594694.5000\n",
            "Epoch 90/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 399695904768.0000 - mae: 540978.1250 - val_loss: 552504262656.0000 - val_mae: 594694.5000\n",
            "Epoch 91/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 376384028672.0000 - mae: 529911.1875 - val_loss: 552504262656.0000 - val_mae: 594694.3750\n",
            "Epoch 92/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 430826881024.0000 - mae: 557453.4375 - val_loss: 552504131584.0000 - val_mae: 594694.3125\n",
            "Epoch 93/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 389284003840.0000 - mae: 540215.6875 - val_loss: 552504066048.0000 - val_mae: 594694.3125\n",
            "Epoch 94/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 459183882240.0000 - mae: 568202.0000 - val_loss: 552504000512.0000 - val_mae: 594694.1875\n",
            "Epoch 95/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 397374881792.0000 - mae: 541707.4375 - val_loss: 552504000512.0000 - val_mae: 594694.1250\n",
            "Epoch 96/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 421778161664.0000 - mae: 561170.1875 - val_loss: 552503869440.0000 - val_mae: 594694.0625\n",
            "Epoch 97/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 427821498368.0000 - mae: 556339.2500 - val_loss: 552503869440.0000 - val_mae: 594693.9375\n",
            "Epoch 98/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 437293514752.0000 - mae: 557262.3125 - val_loss: 552503738368.0000 - val_mae: 594693.8750\n",
            "Epoch 99/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 419964747776.0000 - mae: 547244.8125 - val_loss: 552503672832.0000 - val_mae: 594693.8750\n",
            "Epoch 100/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 404364886016.0000 - mae: 553527.4375 - val_loss: 552503607296.0000 - val_mae: 594693.6875\n",
            "Epoch 101/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 409834848256.0000 - mae: 553261.1250 - val_loss: 552503607296.0000 - val_mae: 594693.6875\n",
            "Epoch 102/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 377787056128.0000 - mae: 532728.0625 - val_loss: 552503476224.0000 - val_mae: 594693.6250\n",
            "Epoch 103/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 436414709760.0000 - mae: 563459.4375 - val_loss: 552503476224.0000 - val_mae: 594693.5625\n",
            "Epoch 104/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 459167432704.0000 - mae: 572312.2500 - val_loss: 552503410688.0000 - val_mae: 594693.3750\n",
            "Epoch 105/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 470358065152.0000 - mae: 582348.0625 - val_loss: 552503279616.0000 - val_mae: 594693.3750\n",
            "Epoch 106/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 396368281600.0000 - mae: 541923.3750 - val_loss: 552503214080.0000 - val_mae: 594693.3125\n",
            "Epoch 107/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 475747057664.0000 - mae: 574900.0625 - val_loss: 552503214080.0000 - val_mae: 594693.1875\n",
            "Epoch 108/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 470595960832.0000 - mae: 574526.7500 - val_loss: 552503083008.0000 - val_mae: 594693.1250\n",
            "Epoch 109/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 402713804800.0000 - mae: 549615.3750 - val_loss: 552502951936.0000 - val_mae: 594693.0625\n",
            "Epoch 110/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 380384018432.0000 - mae: 539893.1875 - val_loss: 552502951936.0000 - val_mae: 594693.0000\n",
            "Epoch 111/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 480970014720.0000 - mae: 583794.1875 - val_loss: 552502951936.0000 - val_mae: 594692.8750\n",
            "Epoch 112/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 398389903360.0000 - mae: 539750.0000 - val_loss: 552502755328.0000 - val_mae: 594692.8125\n",
            "Epoch 113/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 375949950976.0000 - mae: 532044.2500 - val_loss: 552502755328.0000 - val_mae: 594692.7500\n",
            "Epoch 114/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 395819352064.0000 - mae: 543716.2500 - val_loss: 552502624256.0000 - val_mae: 594692.6250\n",
            "Epoch 115/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 392253145088.0000 - mae: 541144.4375 - val_loss: 552502558720.0000 - val_mae: 594692.5625\n",
            "Epoch 116/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 437997076480.0000 - mae: 564757.6875 - val_loss: 552502493184.0000 - val_mae: 594692.5000\n",
            "Epoch 117/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 453971148800.0000 - mae: 569342.6875 - val_loss: 552502427648.0000 - val_mae: 594692.3750\n",
            "Epoch 118/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 415142117376.0000 - mae: 547684.5000 - val_loss: 552502362112.0000 - val_mae: 594692.3125\n",
            "Epoch 119/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 378455097344.0000 - mae: 538756.8750 - val_loss: 552502231040.0000 - val_mae: 594692.1875\n",
            "Epoch 120/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 451001286656.0000 - mae: 570387.7500 - val_loss: 552502165504.0000 - val_mae: 594692.1250\n",
            "Epoch 121/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 476062318592.0000 - mae: 571884.3750 - val_loss: 552502165504.0000 - val_mae: 594692.1250\n",
            "Epoch 122/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 418657304576.0000 - mae: 560272.8125 - val_loss: 552502034432.0000 - val_mae: 594692.0000\n",
            "Epoch 123/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 408942084096.0000 - mae: 550142.0000 - val_loss: 552501968896.0000 - val_mae: 594691.8750\n",
            "Epoch 124/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 485487214592.0000 - mae: 582128.1875 - val_loss: 552501837824.0000 - val_mae: 594691.8125\n",
            "Epoch 125/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 446206050304.0000 - mae: 562791.7500 - val_loss: 552501772288.0000 - val_mae: 594691.7500\n",
            "Epoch 126/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 449448017920.0000 - mae: 565300.1875 - val_loss: 552501706752.0000 - val_mae: 594691.6250\n",
            "Epoch 127/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 437904146432.0000 - mae: 561176.4375 - val_loss: 552501706752.0000 - val_mae: 594691.6250\n",
            "Epoch 128/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 426212950016.0000 - mae: 559964.0625 - val_loss: 552501575680.0000 - val_mae: 594691.4375\n",
            "Epoch 129/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 432228139008.0000 - mae: 558481.6250 - val_loss: 552501444608.0000 - val_mae: 594691.3750\n",
            "Epoch 130/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 436418510848.0000 - mae: 562350.5000 - val_loss: 552501379072.0000 - val_mae: 594691.3125\n",
            "Epoch 131/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 433794383872.0000 - mae: 566424.1875 - val_loss: 552501313536.0000 - val_mae: 594691.1250\n",
            "Epoch 132/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 443376533504.0000 - mae: 563018.2500 - val_loss: 552501248000.0000 - val_mae: 594691.0625\n",
            "Epoch 133/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 454129975296.0000 - mae: 560475.0000 - val_loss: 552501182464.0000 - val_mae: 594691.0000\n",
            "Epoch 134/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 417166000128.0000 - mae: 553083.3125 - val_loss: 552501051392.0000 - val_mae: 594690.8750\n",
            "Epoch 135/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 484646060032.0000 - mae: 585805.9375 - val_loss: 552501051392.0000 - val_mae: 594690.8750\n",
            "Epoch 136/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 412395241472.0000 - mae: 553307.0625 - val_loss: 552500854784.0000 - val_mae: 594690.8125\n",
            "Epoch 137/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 420594286592.0000 - mae: 554115.4375 - val_loss: 552500854784.0000 - val_mae: 594690.6250\n",
            "Epoch 138/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 461011943424.0000 - mae: 569364.1875 - val_loss: 552500789248.0000 - val_mae: 594690.5625\n",
            "Epoch 139/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 441826246656.0000 - mae: 566695.5000 - val_loss: 552500592640.0000 - val_mae: 594690.3750\n",
            "Epoch 140/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 399492710400.0000 - mae: 547623.3125 - val_loss: 552500592640.0000 - val_mae: 594690.3750\n",
            "Epoch 141/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 473194332160.0000 - mae: 581489.7500 - val_loss: 552500527104.0000 - val_mae: 594690.3125\n",
            "Epoch 142/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 393718104064.0000 - mae: 545358.7500 - val_loss: 552500330496.0000 - val_mae: 594690.1875\n",
            "Epoch 143/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 459859460096.0000 - mae: 571591.0625 - val_loss: 552500330496.0000 - val_mae: 594690.0625\n",
            "Epoch 144/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 489669361664.0000 - mae: 595538.8750 - val_loss: 552500264960.0000 - val_mae: 594689.9375\n",
            "Epoch 145/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 420595695616.0000 - mae: 550875.5000 - val_loss: 552500068352.0000 - val_mae: 594689.8750\n",
            "Epoch 146/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 403631472640.0000 - mae: 549740.1875 - val_loss: 552500068352.0000 - val_mae: 594689.8125\n",
            "Epoch 147/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 463041429504.0000 - mae: 580319.3750 - val_loss: 552499937280.0000 - val_mae: 594689.6875\n",
            "Epoch 148/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 400690872320.0000 - mae: 535017.5000 - val_loss: 552499871744.0000 - val_mae: 594689.6250\n",
            "Epoch 149/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 405152169984.0000 - mae: 544434.5000 - val_loss: 552499740672.0000 - val_mae: 594689.5000\n",
            "Epoch 150/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 397976797184.0000 - mae: 546590.4375 - val_loss: 552499740672.0000 - val_mae: 594689.4375\n",
            "Epoch 151/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 395481907200.0000 - mae: 535625.2500 - val_loss: 552499609600.0000 - val_mae: 594689.3125\n",
            "Epoch 152/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 390118440960.0000 - mae: 538106.4375 - val_loss: 552499544064.0000 - val_mae: 594689.1875\n",
            "Epoch 153/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 468019281920.0000 - mae: 575749.5000 - val_loss: 552499478528.0000 - val_mae: 594689.1250\n",
            "Epoch 154/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 416447332352.0000 - mae: 559428.5625 - val_loss: 552499281920.0000 - val_mae: 594688.9375\n",
            "Epoch 155/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 434089984000.0000 - mae: 556368.1250 - val_loss: 552499281920.0000 - val_mae: 594688.8750\n",
            "Epoch 156/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 422999162880.0000 - mae: 547842.8750 - val_loss: 552499150848.0000 - val_mae: 594688.8125\n",
            "Epoch 157/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 421291130880.0000 - mae: 558100.8750 - val_loss: 552499085312.0000 - val_mae: 594688.6875\n",
            "Epoch 158/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 522167287808.0000 - mae: 597003.1250 - val_loss: 552499019776.0000 - val_mae: 594688.6250\n",
            "Epoch 159/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 433996431360.0000 - mae: 563475.3750 - val_loss: 552498888704.0000 - val_mae: 594688.5000\n",
            "Epoch 160/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 420320542720.0000 - mae: 565750.4375 - val_loss: 552498757632.0000 - val_mae: 594688.4375\n",
            "Epoch 161/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 433652662272.0000 - mae: 563426.5625 - val_loss: 552498757632.0000 - val_mae: 594688.3125\n",
            "Epoch 162/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 491742986240.0000 - mae: 592823.0625 - val_loss: 552498561024.0000 - val_mae: 594688.1875\n",
            "Epoch 163/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 457853632512.0000 - mae: 566060.1875 - val_loss: 552498561024.0000 - val_mae: 594688.1250\n",
            "Epoch 164/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 423140130816.0000 - mae: 558351.8750 - val_loss: 552498429952.0000 - val_mae: 594688.0000\n",
            "Epoch 165/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 453597102080.0000 - mae: 571508.5000 - val_loss: 552498298880.0000 - val_mae: 594687.8750\n",
            "Epoch 166/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 447295455232.0000 - mae: 569765.2500 - val_loss: 552498233344.0000 - val_mae: 594687.6875\n",
            "Epoch 167/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 428298371072.0000 - mae: 556576.3125 - val_loss: 552498036736.0000 - val_mae: 594687.6250\n",
            "Epoch 168/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 466977914880.0000 - mae: 576343.5000 - val_loss: 552498036736.0000 - val_mae: 594687.5625\n",
            "Epoch 169/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 438364012544.0000 - mae: 563669.5000 - val_loss: 552497971200.0000 - val_mae: 594687.5000\n",
            "Epoch 170/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 428728418304.0000 - mae: 557381.0625 - val_loss: 552497840128.0000 - val_mae: 594687.3750\n",
            "Epoch 171/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 386429517824.0000 - mae: 546320.8750 - val_loss: 552497709056.0000 - val_mae: 594687.2500\n",
            "Epoch 172/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 464909631488.0000 - mae: 572924.0625 - val_loss: 552497643520.0000 - val_mae: 594687.1250\n",
            "Epoch 173/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 435861880832.0000 - mae: 566152.0625 - val_loss: 552497512448.0000 - val_mae: 594687.0000\n",
            "Epoch 174/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 440163762176.0000 - mae: 561696.8125 - val_loss: 552497446912.0000 - val_mae: 594686.8750\n",
            "Epoch 175/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 451175677952.0000 - mae: 568542.6875 - val_loss: 552497381376.0000 - val_mae: 594686.8125\n",
            "Epoch 176/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 423124369408.0000 - mae: 559705.0000 - val_loss: 552497250304.0000 - val_mae: 594686.6250\n",
            "Epoch 177/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 373489205248.0000 - mae: 527718.3125 - val_loss: 552497184768.0000 - val_mae: 594686.6250\n",
            "Epoch 178/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 444667265024.0000 - mae: 569500.7500 - val_loss: 552497053696.0000 - val_mae: 594686.4375\n",
            "Epoch 179/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 458305273856.0000 - mae: 569297.7500 - val_loss: 552496988160.0000 - val_mae: 594686.3750\n",
            "Epoch 180/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 420967317504.0000 - mae: 552294.8750 - val_loss: 552496857088.0000 - val_mae: 594686.2500\n",
            "Epoch 181/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 466341068800.0000 - mae: 577376.3125 - val_loss: 552496791552.0000 - val_mae: 594686.1250\n",
            "Epoch 182/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 458094346240.0000 - mae: 583365.3750 - val_loss: 552496660480.0000 - val_mae: 594686.0000\n",
            "Epoch 183/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 393753460736.0000 - mae: 540388.6250 - val_loss: 552496594944.0000 - val_mae: 594685.8750\n",
            "Epoch 184/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 420880154624.0000 - mae: 558555.0000 - val_loss: 552496398336.0000 - val_mae: 594685.8125\n",
            "Epoch 185/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 397065519104.0000 - mae: 544765.4375 - val_loss: 552496398336.0000 - val_mae: 594685.6875\n",
            "Epoch 186/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 434163613696.0000 - mae: 559660.7500 - val_loss: 552496201728.0000 - val_mae: 594685.6250\n",
            "Epoch 187/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 439673225216.0000 - mae: 561877.1250 - val_loss: 552496136192.0000 - val_mae: 594685.3750\n",
            "Epoch 188/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 436977172480.0000 - mae: 558901.8125 - val_loss: 552496070656.0000 - val_mae: 594685.3750\n",
            "Epoch 189/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 421201313792.0000 - mae: 556339.1250 - val_loss: 552495874048.0000 - val_mae: 594685.1875\n",
            "Epoch 190/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 449711308800.0000 - mae: 567059.3750 - val_loss: 552495874048.0000 - val_mae: 594685.1250\n",
            "Epoch 191/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 440730812416.0000 - mae: 563645.7500 - val_loss: 552495677440.0000 - val_mae: 594685.0000\n",
            "Epoch 192/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 396911443968.0000 - mae: 548677.6875 - val_loss: 552495611904.0000 - val_mae: 594684.8750\n",
            "Epoch 193/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 424308867072.0000 - mae: 559903.8125 - val_loss: 552495480832.0000 - val_mae: 594684.8125\n",
            "Epoch 194/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 425563783168.0000 - mae: 554067.6250 - val_loss: 552495349760.0000 - val_mae: 594684.6875\n",
            "Epoch 195/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 495375351808.0000 - mae: 591125.3125 - val_loss: 552495284224.0000 - val_mae: 594684.5625\n",
            "Epoch 196/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 466039832576.0000 - mae: 581651.3750 - val_loss: 552495218688.0000 - val_mae: 594684.3750\n",
            "Epoch 197/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 392404369408.0000 - mae: 540036.5625 - val_loss: 552495087616.0000 - val_mae: 594684.3125\n",
            "Epoch 198/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 424379252736.0000 - mae: 563161.3750 - val_loss: 552494956544.0000 - val_mae: 594684.1875\n",
            "Epoch 199/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 404118044672.0000 - mae: 537134.1250 - val_loss: 552494825472.0000 - val_mae: 594684.0625\n",
            "Epoch 200/200\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 429014482944.0000 - mae: 556057.8750 - val_loss: 552494759936.0000 - val_mae: 594683.8750\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 310177267712.0000 - mae: 499561.4375 \n",
            "Test MAE: 488628.0\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Predicted House Price: 22.706335\n"
          ]
        }
      ]
    }
  ]
}